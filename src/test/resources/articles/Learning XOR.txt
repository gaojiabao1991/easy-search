University of Stirling
Department of Computing Science and Mathematics

Computing Science Technical Report

Learning XOR: exploring the space of a
classic problem


 “One could almost write a book about the process of solving XOR …” (McClelland
and Rumelhart 1988, page 303)

Richard Bland

June 1998
Table of contents

    1.  Background..............................................................................................3
    2.  Neural Nets and the XOR problem..........................................................4
      a)   Neural nets                                           4
      b)   Perceptrons and XOR                                   6
    3.  The space of XOR .................................................................................11
      a)   Introduction                                         11
      b)   Boolean Class                                        11
      c)   Boolean Classes as regions in space                  12
      d)   Using the sigmoid function                           15
      e)   Solutions, local minima, and saddlepoints: theoretical issues 17
    4.  The error surface....................................................................................19
      a)   Previous work                                        19
      b)   Where the solutions are located                      20
      c)   Points near an escarpment                            27
      d)   Plateaux and trenches                                32
      e)   Connections to solutions                             43
      f)   Claimed local minima                                 47
    5.  Conclusions ...........................................................................................50
    6.  Appendix: The monotonic connection algorithm..................................51
    7.  Reference List........................................................................................53


                                   2
1. Background
Boolean functions of two inputs are amongst the simplest of all functions, and the
construction of simple neural networks that learn to compute such functions is one of
the first topics discussed in accounts of neural computing.  Of these functions, only
two pose any difficulty: these are XOR and its complement.  XOR occupies, therefore,
a historic position.  It has long been recognised that simple networks often have
trouble in learning the function, and as a result their behaviour has been much
discussed, and the ability to learn to compute XOR has been used as a test of variants
of the standard algorithms.  This report puts forward a framework for looking at the
XOR problem, and, using that framework, shows that the nature of the problem has
often been misunderstood.
The report falls into three main parts.  Firstly, in Section 2 I review neural nets in
general and the XOR problem in particular.  This section is based on classic studies
and the material will be familiar to most readers.  Secondly, in Section 3 I look in
considerable detail at the problem-space of the XOR problem using the simplest
network that can solve the problem using only forward transmission with each layer
communicating only with the next layer.  Finally, in Section 4 I give a comprehensive
account of the error surface.  I show that this surface has exactly sixteen minima and
that each is a solution to the XOR problem.  This is a new result and corrects errors
made by many previous authors.


                                   3
2. Neural Nets and the XOR problem

a) Neural nets
Neural nets are built from computational units such as that shown in Figure 1.  This
unit is a rough analogue of the animal neuron, which connects to other neurons or
biological devices at synapses: these connections are inputs except for a single output
down the axon of the neuron.  The sensitivity of the neuron to its inputs is variable.
The output is all-or-nothing: the neuron either fires or produces no output.
The design of the corresponding computational unit follows this description fairly
closely.  The unit has a number of inputs: these may be from the outside world or
from some other unit.  Each input has an associated weight.  Each unit is considered to
receive a single input stimulus made up of the weighted sum of the inputs.  Writing

 a j  as the jth input (or activation) and wj  as the corresponding weight, we write the
summed input to the ith unit as
          =
       ini  ∑wjia j                                               (1)
             j
The unit then applies a function g to the summed input.  This function is typically a
step function such as
               1, if x ≥ t 
       step(x) =                                                (2)
               0, otherwise
where t is the threshold of the function.  As we shall see, for many purposes it is
important that the function is differentiable, and for this reason the sigmoid function


                      Figure 1: A Unit of a neural net

                                   4
                             1

                           0.75

                           0.5

                           0.25

                             0
       -4    -3   -2    -1    0     1    2     3     4

                        Figure 2: Sigmoid function

                      1
       sigmoid(x) =                                               (3)
                  1+ e −k ( x−t )
is convenient.  Here again the parameter t gives a threshold for the function: the
function approaches 0.0 as x → −∞  and 1.0 as x → ∞ , and for t = 0  takes on the
value 0.5 at x = 0 .  The parameter k affects the steepness of the transition between 0
and 1 in the centre of the curve.  Figure 2 shows this function for k = 1 and t = 0 .
We shall use these values throughout this paper.
Applying the function g to the summed inputs gives us the activation value (the
output) of the ith unit:
           =   ∑
        ai   gi ( w ji a j )                                      (4)
                j
where the subscript on the g indicates that each unit may have its own function:
typically, the functions differ only in the threshold values, t, and it is convenient to
remove this difference by giving each unit an extra input whose value is always − 1.
The weight given to this input thus has an effect identical to that of the parameter t in
Equation (3), and using this weight in preference to manipulating t is algorithmically
simpler.  (This extra input is sometimes referred to as the bias unit.)
These units can represent (some) Boolean functions.  For example, if g is the step
function of Equation (2), then the unit shown in Figure 3 computes the function  X
AND Y, where X and Y are the two variable inputs, X ,Y ∈ {0,1}, with 0 representing
False and 1 representing True.  The bias unit is the right-hand input, which together
with its weight gives the function g a threshold of 1.5.  A more compact
representation of this is given in Figure 4, where the threshold has been written inside
the circle representing the unit.
These units may be combined into networks by assembling them together in layers of
units in parallel, and/or by making the outputs of one unit serve as inputs to other
units.  A network in which there are no cycles is called a feed-forward network: we
discuss only feed-forward networks in this report.  Units whose outputs serve only as
input to other units, and whose behaviour is not directly visible in the outside world,
are said to be hidden.  A feed-forward network containing no hidden units (which
must therefore be single-layer) is called a perceptron (Minsky and Papert 1969).  Thus
Figure 1 is a representation of a single-unit perceptron.


                                   5
