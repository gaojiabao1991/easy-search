使用 mahout 机器学习改进 solr 查询结果

                     版本号：1.0


                     作者:贾德星
                  日期：2012 年 5 月
          山东浪潮齐鲁软件产业股份有限公司
1   概述

    搜索引擎可以通过用户点击与用户的查询输入进行交互，用户与搜索引擎的交互行为反

映了用户的兴趣和主观目的。通过分析用户输入查询之后的点击行为，采用机器学习的方法，

可以优化、改进搜索引擎的查询结果。

    本文将介绍如何使用 mahout 的贝叶斯分类算法，来实现动态调整 solr 查询结果评分的

机制。

2   分类算法与用户点击模型

2.1 贝叶斯定理公式

    贝叶斯定理公式：P(A|B)=P(B|A)*P(A)/P(B)

    用于计算在已知 B 的条件下 A 发生额概率。

    P(A)观察到概念 A 的概率，称为先验概率，在 mahout 中 A 称为 Label。

    P(B/A）从属于 A 的实例中随机选择一个实例，选到 B 的概率，也称为似然率。

    P(B)在一般情况下选到 B 的概率，在 mahout 中 B 称为 Feature。

2.2 朴素贝叶斯算法

    设每个数据样本用一个 n 维特征向量来描述 n 个属性的值，即：X={x1，x2，…，xn}，

假定有 m 个类，分别用 C1,        C2,…，Cm 表示。给定一个未知的数据样本 X（即没有类标号），

若朴素贝叶斯分类法将未知的样本 X 分配给类 Ci，则一定是

        P(Ci|X)>P(Cj|X) 1≤j≤m，j≠i

    根据贝叶斯定理

    由于 P(X)  对于所有类为常数，最大化后验概率 P(Ci|X)                 可转化为最大化先验概           率

P(X|Ci)P(Ci)。如果训练数据集有许多属性和元组，计算 P(X|Ci)的开销可能非常大，为此，

通常假设各属性的取值互相独立，这样

    先验概率 P(x1|Ci)，P(x2|Ci)，…，P(xn|Ci)可以从训练数据集求得。
    根据此方法，对一个未知类别的样本 X，可以先分别计算出 X 属于每一个类别 Ci 的概

率 P(X|Ci)P(Ci)，然后选择其中概率最大的类别作为其类别。

    朴素贝叶斯算法成立的前提是各属性之间互相独立。当数据集满足这种独立性假设时                                    ,

分类的准确度较高，否则可能较低。另外，该算法没有分类规则输出。

2.3 用户点击模型

    在用户搜索、点击结果的行为中主要有 3 个关键属性：

    �   用于标识用户的字符串

    �   用户搜索请求字符串

    �   用户在查看搜索结果后选择点击的文档 ID

    我们的需求是：用户输入搜索请求、得到搜索结果后，最可能选择点击的是哪些文档。

由此我们将文档 ID 做为 A--Label，查询请求分词后的字符串做为 B--Featrue，实现在已得知

B，判断它属于 A 的概率，即 P(A/B)。

    Bayes 分类算法需要线下训练数据           (来自搜索引擎日志)来进行学习后建立模型，然后根

据建立的模型和用户输入的搜索请求在线计算得到各文档可能被点击的概率，并进行评分、

排名。

3   环境


    部署 solr 应用+自扩展的搜索引擎日志处理器及组件。

    部署 mahout。

    由于 mahout 的很多机器学习算法都是基于 hadoop              map/reduce，故还需要部署 hadoop

环境。

4   搜索引擎日志

    用户点击日志的数据模型：

   属性名              属性值                               说明

ip           用户 IP 地址             需要前端代理通过参数传递到日志组件
session      会话标识                 可记录在 cookie 中，需要通过参数传递

date         点击时间                 用户点击结果的时间

query        用户查询词语               未分词的 q 参数值

terms        已分词的查询词?

doc          用户点击的文档标识            使用业务标识，不能使用 docid

seqnum       序号                   所点击文档在结果中的序号


    其他我们主要使用了 terms 属性和 doc 属性。

    针对 solr 我们开发了 solr.SearchHandlerLog 处理器和 solr.JDBCLogComponent

组件，使用关系数据库记录用户查询、点击的日志。

5   准备数据


    Mahout 的 Bayes 分类算法是一种分布式的算法，支持在              hadoop 上执行   map/reduce 运算，

解析文本文件来建立 bayes 分类模型。

    因此需要把 sql 数据库中的日志记录导出为如下文本格式：

    label\tterm_1 term_2 ... term_n\n

    每行一条数据，包括 label 和 feature 集，label 和 feature 集之间以\t 分隔，feature 集内之

间以空格分隔，term 就是使用分词器对查询请求分词后的多个 term 串。

    主要代码：
File  outfile = new File("userclick.log");
rs =  stmt.executeQuery("SELECT  DOC,TERMS  FROM SOLR_LOGDATA  where
LOGTYPE='CLICK'"  );

writer  = Files.newWriter(outfile,   Charset.forName("UTF-8"));
while(rs.next()){
    String  label = rs.getString(1);
    String  feature = rs.getString(2);
    if(label  == null || feature  == null)
        continue;
    writer.write(label);
    writer.write("\t");
    writer.write(feature);
    writer.write("\n");
}


    当然也可以在 solr 中使用 FileLogComponent，直接使用 mahout 文件格式。

    训练数据准备完毕需上传至 hdfs，如：/solr/log。

6   建立模型


    在 mahout 部署机器上执行：

    bin/mahout  trainclassifier  -i /solr/log -o /solr/model  -type bayes -ng

1 -source  hdfs

    -i : 训练数据输入目录，位于 hdfs 系统

    -o : 学习模型的输出目录，位于 hdfs 系统

    -type ：  分类算法类型，本处使用 bayes 算法

    -ng ：  特征集的跨度，即几个 term 做为一个特征

    -source  ： 数据来源类型，暂只支持 hdfs

    实际执行中根据部署环境，可能需要修改 mahout 脚本文件，设置环境变量，如：

    export  HADOOP_CONF_DIR=/opt/hadoop-0.20.2/conf

    export  JAVA_HOME=/opt/jdk1.6.0_24

    export  HADOOP_HOME=/opt/hadoop-0.20.2


    执行完毕，在/solr/model 目录下将会看到已经建立的分类算法模型，可能包括如下

文件：

    trainer-tfIdf/trainer-tfIdf/part*

    trainer-weights/Sigma_j/part*

    trainer-weights/Sigma_k/part*

    trainer-weights/Sigma_kSigma_j/part*

    trainer-thetaNormalizer/part*
